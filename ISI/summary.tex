\documentclass[9pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{listings}
\usepackage{xcolor}
%\usepackage{enumitem}

\oddsidemargin 0mm
\evensidemargin 0mm
\topmargin 0mm
\textheight 216mm
\textwidth 165mm

\pagestyle{fancy}
\fancyhf{}
\fancyhead[CE,CO]{ISI paper evaluation by Ivaylo Petrov and Hristina Hristova}
\fancyfoot[CE,CO]{\thepage}

\lstset{basicstyle=\ttfamily,
  mathescape=true,
  escapeinside=||,
  emph={Task, Operation, for, each, if, else, endif, endfor, while, then, When,
  do, repeat, until, endrepeat},
  emphstyle={\color{gray}\bfseries\itshape}}

%\setlist[itemize]{noitemsep,nolistsep}
%\setlist[enumerate]{noitemsep,nolistsep}

\def\CC {{\mathbb C}}        
\def\RR {{\mathbb R}}       
\def\ZZ {{\mathbb Z}}      
\def\NN {{\mathbb N}}     
\def\be  {\begin{eqnarray}} 
\def\ee  {\end{eqnarray}}  
\def\ben {\begin{eqnarray*}} 
\def\een {\end{eqnarray*}}   
\newcommand{\hr}{\rule{\linewidth}{0.1mm}}
\newcommand{\bighs}{\hspace{15pt}}
\newcommand{\hs}{\hspace{10pt}}
\renewcommand{\tilde}{\overset{-}}

\newenvironment{itemize*}{
  \begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
}{
  \end{itemize}
}

\newenvironment{enumerate*}{
  \begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
}{
  \end{enumerate}
}

\newtheoremstyle{plain}{1pt}{0pt}{}{}{}{}{.5em}{\thmname{\textbf{#1}}:\thmnote{#3}}

\theoremstyle{plain}

\everymath{\displaystyle}

\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}}

\begin{document}

\bibliographystyle{plain}

\section*{\centering
  Paper evaluation of\\
  \emph{Strategies for green data-processing frameworks}\\
  by\\
  Ivaylo Petrov and Hristina Hristova
}

\hr

In the recent years MapReduce is de-facto the standard for computations on
big amounts of data. Because of that in this report we are going to present two
different strategies for achieving energy efficiency for data centers that run
MapReduce tasks. The first approach concerns MapReduce with Interactive Analysis
work flows (MIA) which include interactive services, traditional batch
processing, and latency-sensitive processing. The authors of [1] have presented
BEEMR (Berkley Energy Efficient MapReduce) method inspired by the empirical
analysis of real-life MIA traces. The second one we describe how to manage a
data center to use maximum amount of "green" energy and if we have to use
"brown" energy, how to optimize the amount of money we have to pay for it. All
that is implemented in GreenHadoop \cite{greenhadoop}

The researches of [1] have chosen to use Facebook work flows as a case study. Analysis
on the traces of the system's primary Hadoop cluster have been made. After performing
clustering techniques on the different types of jobs in the workload, the following
results have been yielded: while most of the jobs are interactive and small (ad-hoc
queries) there are jobs with much longer duration which take a lot of time to complete.
Moreover, the accessed data sets are small in most of the cases, periods of peaks of the work flow
are observed along with periods of low activity which nevertheless run at near-to-peak
power.

The built BEEMR method is a design approach aiming to reduce the number of
machines which serve the interactive jobs while running the other less-time demanding
in a batch fashion. What is done is actually the splitting of the cluster into
two zones: interactive zone which is fully powered and is made up of small percentage
of the cluster resources, and batch zone which is formed from the rest of cluster
and is powered low between the batches. There are also three kinds of jobs. When
a job is classified by BEEMR as interactive, it has to be serviced with low latency.
This type of jobs are the ones which input data is below some threshold. The jobs
which are too long are classified as interruptible. The rest of the jobs are batch jobs.
The interactive zone executes every interactive task that has accessed. This is
not the case with the interruptible jobs and the batch ones. Their tasks are put in a waiting queue.
At particular times, the batch zone is fully powered and runs the tasks that are in
the queue. In a new task arrives in the waiting queue during the execution of the
batch, it has to wait for the next batch. Once all batch tasks finish, the interruptible
ones are returned to the waiting queue and the batch zone is turned to low power again.
It is possible for some batch task not to complete - in this case the cluster remains fully
powered for the next batch periods. A bit of a challenge is that such batch tasks that
do not finish until the end of a batch may cause its incompletion.

The optimization process involves several parameters: the total number of map/reduce
task slots, the map slots to reduce slots ratio, the percentage of the cluster for the
interactive zone, the thresholds for classifying a job as interactive and as interruptible,
the length of the batch interval and the algorithm for determining the number of
map/reduce tasks to assign to a job. The following algorithms are considered - Default which
is the default setting in Hadoop, Actual which is based on settings at Facebook
and Latency-bound which assigns the jobs in a way that no task will run more than 1 hour.

A simulator was created for evaluation of the methodology. How does it work? Firstly,
it queues and classifies the newly arrived jobs after which it checks for available slots
and assigns them to those tasks which can be run immediately. Later the simulator
returns to the queue the jobs whose tasks have not finished. Finally, if a job has finished,
it is marked as complete.

In the following section we are going to expose the results obtained by the simulator.
The cluster size is selected so that long queues to be avoided and at the
same time the energy consumption not to be high. The results from 45-days trace
show that at least 72000 slots in total (map and reduce) are needed. The batching interval is also an
issue - one very natural solution would be to be changed dynamically. The
experiments point out that map slot occupancy in comparison to the reduce slot one
reaches full capacity for a little time during the batching period, with no further
tasks after that. The low reduce slot capacity causes sometimes the batch zone to be fully powered
during the whole batching period. One of the ways for fixing this is the improving of assigning
the number of task slots per job. The first two of the considered algorithms (Default and Actual)
do not manage to deal with the described problem. The latency-bound policy,
however, results in higher utilization of the cluster during the batch and that
way allows it to complete and to be turned to low-power. Furthermore, by changing the map-to-reduce
ratio, the map slots can be increased and the reduce ones decreased which does not
affect the completion times for the reduce slots only allows the map ones to finish
faster. Here again, the results show that the Latency-bound algorithm achieves
the best energy efficiency level when that approach is used, when compared to the other two algorithms. There is, of course,
a development which can be performed like dynamically adjusting the map-to-reduce
ratio. So far, the threshold for classifying a job as interruptible has been set to
24 hours. The lower its level, the faster the job executions in the batch zone and in the interactive
zone but the higher the latency. The authors have tried to change the threshold to
12 hours and to 6 hours. Of course, that value does not have to be decreased too much -
this would result in infinite queue. For the data from the traces of Facebook an ideal case
for energy saving is estimated and the experiments show that setting the level of the
interruptible threshold to 6 hours recovers to 92\% of the energy from the ideal case.

As far as the latency is concerned, the results are different for the different
kind of jobs. The interactive jobs' latency is unchanged and even a little bit
improved (because of the dedicated resources). The results show that during peaks
of the workload it is required for the capacity of the interactive zone to be increased.
The latency ratio for the batch jobs varies which can be explained by the fact that
if a job arrives after the batch has started its execution, the job will be delayed
and vise versa, if it arrives just before the start of the execution, it has almost no delay.
The first case leads to a latency, the second one - does not. Surprisingly, the interruptible
jobs' latency ratio varies less than that of the batch jobs. The conclusion is
that the interruptible jobs are really long running jobs. The energy savings result in this delayed
execution of batch and interruptible jobs.

The simulator is tested whether it actually fits the reality through using an Amazon EC2
cluster. Both the results from the simulator and from the experiments run on the
cluster are compared. There are some errors observed. The first one comes from the
observation that the simulated job duration does not completely match the one from
the measurement on the real cluster. This is because the simulator underestimates
the run times for small sizes and overestimates the ones for large sizes. The way
the data from the real cluster is summarized also has its effect. The second error
concerns the predicted versus actually energy savings. After adjustments, the
error between the two kinds of energy savings becomes 13\%. This is due to the fact that
the simulator assumes that the times of the tasks are the same when a task arrives
and when it is executed on a batch. The results taken from the real data reveal
that this is not a correct assumption. Still, the conclusion that can is drawn is
that BEEMR saves energy up to 40-50\% of the ideal-case.

\section{GreenHadoop} % (fold)
\label{sec:GreenHadoop}
  In this part GreenHadoop will be presented. The target application is to
  optimize both the amount of renewable or green energy that is used by small or
  medium size data centers and to optimize the price that has to be paid for
  energy drawn from the electrical grid. Big data centers are owned by big
  companies that has the opportunity to optimize the cost efficiency of their
  data centers. It is the smaller ones that are still not very optimal and for
  which a great amount of work should be done.

  In this paper the authors has assumed that the data center that is to optimize
  its green energy utilization has its own solar power plant and it can not use
  the energy produced in any other way except for computations. The reasons for
  those assumptions are well described in the paper (see \cite{greenhadoop}). 

  In order to be able to provide some real improvement, the authors has also
  assumed that some of the jobs can be delayed with up to some finite amount of
  time (generally one day for the simulations) and that the data center is under
  utilized. If it is fully utilized all the time, there is no easy way to
  improve its energy efficiency. By adding the possibility to delay some of the
  task, it is possible to both use green energy if it is expected that it will
  be sufficient for the computations (a prediction of the amount of green energy
  supply is created based on some meteorological information) or use the
  cheapest energy. As energy is cheapest at some night hours when it is much
  less used and there are other factors that contribute to the amount of money
  to be paid for the "brown" energy, it is possible to take also all those
  things into account when deciding when to schedule the tasks that should be
  executed.

  The way the proposed system, GreenHadoop, differs from Hadoop is that it does
  not try to obtain results as soon as possible, using all the available
  servers. That way it is nearly impossible to reduce the amount of power that
  the data center will consume. GreenHadoop, on the other hand, tries to
  determine how to schedule the tasks so that it can use as much green energy as
  possible and also cheap energy, when the "green" one will not be enough. The
  goal is to maximize the number of servers that are turn off, especially when
  the energy has high prices, without endangering the deadlines for each task.
  GreenHadoop has five possible priorities: very high, high, normal, low, and
  very low, that are the same as those in Hadoop. The difference is that in
  GreenHadoop it is possible to postpone a job with priority below high for a
  fixed amount of time.

  During the course of the execution of different tasks, each server in the data
  center can be in one of the following three states: Active, Decommissioned or
  Down. In the Down state it is stopped in a way that consumes only a very small
  amount of energy, but that allows a fast start of that server. Decommissioned
  is a server that is used either to only store some data blocks or server that
  should be put in Down state, on which there are some tasks that still have not
  completed their execution. No new tasks or data blocks are added to
  Decommissioned servers. Active servers are servers that are running tasks on
  full speed.

  The execution can be explained as follows: when a new tasks is received it
  either goes to the waiting queue in FIFO order if its priority is below high,
  or goes to the Hadoop queue in FIFO order otherwise. The time is divided in
  epochs and before the start of the next one, each task received in the
  previous epoch is determined an epoch before which the job has to start
  executing in order the result to be obtained on time. Here it is important to
  note that for each job an estimation of the number of servers and time it will
  take is made, that relies only on some statistics, not on data provided by the
  submitter of the job.

  In the beginning of each epoch the number of Active servers for that epoch is
  also determined. It is computed by the expectation of the energy provided by
  the renewable source of energy, the priorities and deadlines of the jobs. If
  it is possible GreenHadoop will start as much jobs as possible from the
  waiting queue when there is enough green energy. When selecting jobs to be
  started, jobs with priority above normal are directly started and if more
  green energy is available, it is used to run the jobs which are supposed to
  run sooner from the waiting queue and for which all the data they need is
  presented in Active or Decommissioned server. 

  If the number of servers is greater then the needed ones, some of the servers
  are put in Decommissioned state and are eventually put in Down state when
  their data blocks are not needed or are copied to servers that will remain in
  some up state. If, on the other hand, we need more computational power, we
  first check if we can put some of the servers in Decommissioned state in
  Active one and use them otherwise we put some of the Down servers in Active
  state. We can start a Down server also if some data that is presented only on
  it is needed.

  For the testing of the proposed approach two different workloads are chosen -
  "Facebook-Derived" and "Nutch Indexing". Details for the testing environment
  can be found in \cite{greenhadoop}. One of the important things there is that
  the experiments are not in real time, but a 2-day experiments are run in 2
  hours. Although there is some information why this is necessary and how does
  it affect the executions of the experiments, additional details could be
  useful to understand how the real values are supposed to differ from the
  obtained ones.

  In the experiments that the authors of \cite{greenhadoop} performed, no time
  bounds were violated except for the case when the maximum delay was less or
  equal to 12 hours. In the case of 12 hours the delay was about 3\% and most of
  them exceeded the time bounds with only a small amount. There are a number of
  other experiments that study how the amount of "green" energy, data center
  utilization, fraction of high-priority jobs, workloads and energy estimation.
  The results show that the gain from this technique increases with the decrease
  of data center utilization and number of high priority tasks. The improvement
  in green energy utilization is between 20 and 30 percent in most cases and the
  price for brown energy is decreased by 22\% in some cases and up to more then
  60\% in others. On average the authors state that they manage to decrease the
  electricity cost by 39\% and to increase the green energy utilization by 31\%.

  There are a few interesting things that are omitted in this article, however.
  The first of them is if there is some impact of failing nodes to the
  performance of the system. It seems that during their experiments there were
  no failing jobs, no failing servers. This, however, can happen. 

  Also it would be interesting to provide some additional information about
  longer runs, as it is not clear if runs that represent only two days can catch
  all the characteristics of the system when it is run for a year for example.

% section GreenHadoop (end)

\section{Conclution} % (fold)
\label{sec:Conclution}
  To sum up, those techniques can be used to significantly decrease the
  electricity cost for data centers. The authors of \cite{greenhadoop} have
  calculated that if a data center creates its own solar power plant in some
  places of the world, in 10-11 years they will have their money back and the
  rest of the time that the power plant is expected to work (10-20 years), they
  will be saving additional money.
% section Conclution (end)

\begin{thebibliography}{widest entry}
  \bibitem{hrisi} \textcolor{red}{Hrisi to fill here her article and change [1]
    with cite\{something\}}
  \bibitem{greenhadoop} I. Goiri, et al. GreenHadoop: Leveraging green energy in
    data-processing frameworks. In Proc. of EuroSys’12, pages 57–70, 2012.
\end{thebibliography}

\hfill\\

\end{document}
