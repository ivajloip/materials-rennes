\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}

\oddsidemargin 0mm
\evensidemargin 0mm
\topmargin 0mm
\textheight 216mm
\textwidth 165mm

\pagestyle{fancy}
\fancyhf{}
\fancyhead[CE,CO]{VAR paper evaluation by Ivaylo Petrov}
\fancyfoot[CE,CO]{\thepage}

\lstset{basicstyle=\ttfamily,
  mathescape=true,
  escapeinside=||,
  emph={Task, Operation, for, each, if, else, endif, endfor, while, then, When,
  do, repeat, until, endrepeat},
  emphstyle={\color{gray}\bfseries\itshape}}

\setlist[itemize]{noitemsep,nolistsep}
\setlist[enumerate]{noitemsep,nolistsep}

\def\CC {{\mathbb C}}        
\def\RR {{\mathbb R}}       
\def\ZZ {{\mathbb Z}}      
\def\NN {{\mathbb N}}     
\def\be  {\begin{eqnarray}} 
\def\ee  {\end{eqnarray}}  
\def\ben {\begin{eqnarray*}} 
\def\een {\end{eqnarray*}}   
\newcommand{\hr}{\rule{\linewidth}{0.1mm}}
\newcommand{\bighs}{\hspace{15pt}}
\newcommand{\hs}{\hspace{10pt}}
\renewcommand{\tilde}{\overset{-}}

\newenvironment{itemize*}{
  \begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
}{
  \end{itemize}
}

\newenvironment{enumerate*}{
  \begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
}{
  \end{enumerate}
}

\newtheoremstyle{plain}{1pt}{0pt}{}{}{}{}{.5em}{\thmname{\textbf{#1}}:\thmnote{#3}}

\theoremstyle{plain}

\everymath{\displaystyle}

\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}}

\begin{document}

\bibliographystyle{plain}

\section*{\centering
  Paper evaluation of\\
  \emph{"KinectFusion: Real-time 3D Reconstruction and Interaction Using a
    Moving Depth Camera"}\\
  by\\
  Ivaylo Petrov
}

\hr

The following article is going to provide information about \emph{Real-time 3D
Reconstruction and Interaction Using a Moving Depth Camera}. The main focus 
will be on the paper \emph{"KinectFusion: Real-time 3D Reconstruction and
Interaction Using a Moving Depth Camera"} by Shahram Izadi, David Kim,
Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli,
Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison, Andrew Fitzgibbon.

\section{Abstract} % (fold)
\label{sec:Abstract}
  Creation of an relatively accurate 3D map of some objects with a low-cost 
  hand-held device has been a goal of a number of researches. Now with a Kinect
  camera and appropriate software that is possible. Furthermore, with this 
  camera now we are not only able to rapidly create a detailed 3D reconstruction
  of a limited size scene (a room for example) in real time \cite{kinectfusion},
  but we can also use this reconstruction to create geometry-aware augmented
  reality with physical interactions within it. 

  KinectFusion is a software that is capable of using a Kinect camera to do all
  that without the need of additional widely used devices such as trackers. It
  also provides us with the possibility to enable real-time multi-touch
  interactions anywhere, allowing any planar or non-planar reconstructed
  physical surface to be appropriated for touch.
% section Abstract (end)

\section{Introduction} % (fold)
\label{sec:Introduction}
  For quite some time people have been fascinated by the possibility with an 
  affordable, hand-held device and in real time, to scan accurately 3D shapes,  
  to create geometry-aware augmented reality, and to create physically based
  interactions within it. So far, however, this has been impossible. With the
  inventing of the Kinect camera the world is presented with the possibility to
  create relatively accurate 3D maps for very low price. For that reason this
  device rapidly gained popularity among researchers and enthusiasts alike.

  While this device does not provide a conceptually new approach, structured
  light technique to obtain depth map is not a new idea, its low-cost and
  relative accuracy contribute greatly to its success. As the authors of the
  article note, however, the noise that is caused by fluctuations and holes
  where reads were not obtained need to be taken care of with some algorithms in
  order to make the device really useful for the above scenarios. 

  The already described applications of this device, with the appropriate
  software, range from gaming and physics to CAD programs. For all those things,
  however, an accurate model should be inferred from the noisy point-based data.
  To do that the authors consider different approaches and finally decide to use 
  \textbf{volumetric surface representation}. Although this approach has some 
  drawbacks, i.e. hight memory consumption, which might potentially lead to high
  computational cost (as noted in
  \cite{representaion-and-rendering-of-implicit-surfaces}). There however is a
  solution to this problem, that the authors suggest - the use of octree.
  Although there is no additional information about this approach in the
  original article, it will be explained in the appropriate part of this paper. 

  As the system is designed in a way that allows moving of the Kinect camera and
  interaction of users with the scene, the authors needed to solve a number of
  difficulties that those two characteristics introduced. In order to track the
  camera position, they have decided to track the \emph{6 degrees-of-freedom
  (DOF)}. They divide the input in two parts - foreground and background and use
  only the data from the background to calculate the 6DOF, because as algorithms
  generally depend on the staticness of the scene, it is impossible to
  accurately calculate the camera position if all the data is considered. This
  allows complex user interactions with the camera that does not significantly
  degrade the quality of the camera tracking. This division of foreground and
  background objects makes possible the use of the system to detect multi-touch
  interactions with physical surfaces of any type - planar or not.

% section Introduction (end)

\section{Related work} % (fold)
\label{sec:Related work}
  Although scene geometry reconstruction using various techniques such as active
  sensors, passive cameras, online images or from unordered 3D points, is well
  studied in the field of computer graphics and vision, the possibility of
  interaction with the scene is what distinguishes \cite{kinectfusion} from all
  proposed solutions until now. The area of Simultaneous Localization and
  Mapping (SLAM) is also tightly connected with the proposed system, as it has
  to determine its position on in real time as new observations are provided,
  while at the same time create a precise 3D representation of the observed
  objects and cope with user interactions. How this problem is solved in the
  described article will be explained later.

  Other differences between KinectFusion and previous works can be described as:
  \begin{itemize}
    \item \textbf{Interactive rates} In contrast with most of the already
      existing solutions, KinectFusion wants to provide real-time computation of
      both camera position tracking and world geometry reconstruction. This is
      essential for some of the applications that are targeted by the authors
      and most of all for games and augmented reality, where direct feedback and
      user interactions are key to the usability of the system. By the time that
      the article was published, there was no system, to the knowledge of the
      authors, that provided interactive rates for those two things.
    \item \textbf{No explicit feature detection} Unlike some of the previously
      created techniques that use Structure from Motion (SfM) or RGB plus depth
      (RGBD) detect sparse scene features, the proposed solution uses directly
      in some way all the data acquired by the Kinect sensor. This also makes
      the system independent of the light conditions in the observed scene.
    \item \textbf{High-quality geometry reconstruction} The authors of the
      system, unlike many other before them, do not want to sacrifice details in
      the reconstructed model in order to obtain real time tracking. Often,
      sparse maps are used for the localization, but they can not be used as a
      mean to reconstruct the observed geometry. Instead of that, the authors
      use volumetric surface representation, which enables them to directly
      reconstruct surfaces that can also be used for rendering of the scene,
      possibly with objects from augmented reality, in real time.
    \item \textbf{User interaction assumed} In most of the previously provided
      studies, the scene was expected to be completely static in order for the
      systems to work. The authors of \cite{kinectfusion} created a system that
      along with tracking the camera and reconstructing the scene, allow user
      interaction. This requires the selection of an representation, that can
      easily deal with this dynamicity, without compromising the real-time
      nature of the computations.

      It is also mentioned that systems that use ToF, which stands for
      Time-of-Flight does not deal with dynamic scenes. This is peculiar, as the
      technology of ToF is just used to obtain depth map. There are two ways
      those maps to be obtained - through devices that use ToF, which however is
      currently too pricey, or projecting a known infrared pattern onto the
      scene and determining depth based on the pattern's deformation
      \cite{body-scanning}. The later is, however, much cheaper. With this said,
      it is not clear why the authors of \cite{kinectfusion} mix depth map
      observation with dynamic scene. There might have been a reason for that,
      but it should have been better described.
    \item \textbf{Mobility without additional infrastructure} The idea of the
      project is to create mobile (at least to some degree) system that can be
      used to reconstruct any indoor spaces. This distinguishes the proposed
      solution from many earlier works that used fixed or large sensors, or
      sensors that are fully embedded in the environment. Also in order the
      application to easier to use, the authors wanted to avoid using
      augmentation of the space and the interacting entities with some markers.
    \item \textbf{Size scale} Additional requirement that distinguishes the
      proposed approach is the support of bigger scenes - a whole room for
      example. In the past most of the detailed reconstruction techniques that
      were proposed would be usable only for relatively small geometries.
    \item \textbf{Low-cost scanning} The combination of low-cost, real-time and
      excellent geometry details has not been shown by any of the previous
      works. Those things contribute to much better user experience, as the user
      can instantly verify that the scanned object has the necessary properties
      and proceed with the use of the created model. Furthermore, the system can
      be used not only to scan static scenes, but also to scan objects that are
      rotated by the user in front of the camera. For the latter, however, there
      are a few requirements that should be satisfied in order this use to be
      applicable. First of all, the object should occupy a sufficiently large
      portion of the depth map in order ICP to be able to converge correctly.
      The moving of the object should not be too quick, as this might cause
      problems to the tracking algorithm as well.

      It is still possible to scan smaller object, provided that first the
      system is able to reconstruct sufficiently accurate representation of the
      surrounding world and just after that the object is presented for
      scanning.
  \end{itemize}
% section Related work (end)

\section{KinectFusion} % (fold)
\label{sec:KinectFusion}
  Following the example of the example of the authors of \cite{kinectfusion}, we
  will now present a number of applications of the KinectFusion system and after
  that some of its parts will be described in more details and additional
  information will be provided from
  \cite{representaion-and-rendering-of-implicit-surfaces} and \cite{icp}.

  The system tries to constantly keep track of the camera position in the space,
  calculating constantly the 6DOF and adding new data as camera is moved or new
  observations are added for places, where before that there were holes, i.e.
  there was no information for that part of the scene. The camera motion does
  not have to be too big in order new data to be acquired, this can happen, for
  example, because of a slight camera shake. The authors point out that given
  the real-time speed of the reconstruction, the level of details of the
  reconstructed objects is more then good. More details on that will be provided
  later in this paper. Using the RGB camera that the Kinect sensor has, it is
  also possible to map textures to the reconstructed objects as shown in
  Figure~\ref{fig:model_with_texture}B.

  \begin{figure}[h]
    \centering
    \caption{Model with texture mapped from Kinect RGB camera}
    \includegraphics[height=40mm]{graphics/augented_reality.png}
    \includegraphics[height=40mm]{graphics/model_with_texture.png}
    \includegraphics[height=40mm]{graphics/augmented_reality_with_a_number_of_small_spheres.png}
    \label{fig:model_with_texture}
  \end{figure}

  One of the possible application of the system is for \textbf{creation of
  augmented reality}. It is possible to add some 3D objects in the model,
  created by the system and then during the rendering to show them to the user
  as viewed from the position of the kinect camera, reflecting or casting
  shadows to neighboring objects. The requirement to view the scene from the
  kinect camera position comes from the fact that for texture mapping, the
  system uses the build-in Kinect RGB camera, which is at the same position as
  the depth camera.  In Figure~\ref{fig:model_with_texture}A an additional metal
  sphere can be seen, that is not presented in the physical scene, but that
  looks exactly as if it was at that position during the taking of the picture.

  Another application is \textbf{physical world dynamics simulation}. That means
  that, due to some properties of the implementation that preserve some
  information about the velocity of the objects in the scene, it is possible to
  simulate rigid body collisions between real and virtual objects that are
  physically precise and, of course, real-time. This can have great application
  in areas such as gaming and robotics. The number of virtual objects is not
  precisely limited, it is believed that as much as a few thousands of them can
  be simultaneously introduced in the scene, without breaking the time bounds of
  the execution. Such scene can be viewed in
  Figure~\ref{fig:model_with_texture}C, where there is a big number of small
  spheres that are added to the real scene.

  In order to obtain the velocity information that was previously mentioned, the
  authors of the article divide the space in two parts - foreground and
  background. In the foreground is the interacting and moving user and in the
  background is the static scene. This segmentation is achieved through
  declaration of the pixels where the scene has changed too much as outliers
  that are after that processed and separately rendered and added in the final
  visualization. 

  It is this segmentation that allows relatively rapidly moving foreground
  objects to interact and collide with rigid virtual ones. 

  What is maybe even more interesting about this segmentation is that it
  actually allows us to calculate interactions between foreground objects
  (users) and background scene. The points of interaction indicate the positions
  where the user comes into contact with the scene. This can be used to detect
  touching points between the user and any surface, which in turn can be used to
  allow direct multi-touch interactions (see Figure~\ref{fig:interactions}A and
  B).

  \begin{figure}[h]
    \centering
    \caption{Interactions between user and environment}
    \includegraphics[height=40mm]{graphics/interaction_actual.png}
    \includegraphics[height=40mm]{graphics/virtual_interactoin.png}
    \label{fig:interactions}
  \end{figure}
% section KinectFusion (end)

\section{Implementation} % (fold)
\label{sec:Implementation}
  The implementation of the system has to rely on significant computational
  power for a great number of relatively simple tasks. For that purpose the
  authors decided to use GPUs for the computations as they are specifically
  designed for such applications and as there are possibilities to use them not
  only to do graphical computations, but also to do a wide range of concurrent
  computations, that are relatively independent as is the currently depicted
  case. For the implementation the authors of the article use CUDA (a parallel
  computing platform and programming model created by NVIDIA and implemented by
  the graphics processing units (GPUs) that they produce) in order to run a big
  number of parallel threads simultaneously. 

  The computational process that is presented can be divided in the following
  parts that will be described after that: Depth Map Conversion, Camera
  Tracking, Volumetric Integration, Raycasting (see Figure~\ref{fig:pipeline}).
  Each of them is executed for each new data provided by the sensor on the GPU
  unit.

  \begin{figure}[h]
    \centering
    \caption{Computational pipeline}
    \includegraphics[height=55mm]{graphics/pipeline.png}
    \label{fig:pipeline}
  \end{figure}

  \subsection{Depth Map Conversion} % (fold)
  \label{sub:Depth Map Conversion}
    It is possible to imagine that each observation \emph{i} that we have
    represent the state of the scene at some time $i * m$ where m is the time
    between two consecutive observations. If we denote the depth map as
    $D_i(u)$, where $u = (x, y)$ is a pixel obtained by the infrared camera and
    the calibration matrix of the Kinect infrared camera as K, we can compute
    the depth measurements as a 3D vertex in the camera's coordinate space as
    follows: $v_i(u) = D_i(u) K ^ {-1}[u, 1]$. This can be computed in parallel
    on the GPU. After that normal vectors for each vertex can be computed, again
    in parallel by the GPU, using the neighboring vertexes (see
    \cite{kinectfusion} for details).
    Given that the 6DOF are presented, i.e. a matrix $T_i = [R_i|t_i]$ that
    contains 3x3 rotation matrix($R_i$) and 3D translation vector ($t_i$), it is
    possible to convert the vertexes and the normals from the coordinate space
    of the camera to some global coordinate space.
  % subsection Depth Map Conversion (end)

  \subsection{Camera Tracking} % (fold)
  \label{sub:Camera Tracking}
    In order to track the camera pose for each new depth frame, the authors have
    decided to use the well studied and very popular ICP algorithm (see
    \cite{icp} and \cite{efficient-icp} for more details). Typically this
    algorithm is used for alignment of 3D shapes, but in this case it is used to
    find 6DOF transformation that most closely aligns the current oriented
    points with those of the previous frame. The authors use \emph{projective
    data association} to do this. Those transformations can be incrementally
    applied to give the single global camera position $T_i$.

    While doing this, the algorithm also detects potential outliers by
    calculating distances and angles in the global coordinate system and
    checking that they are within some expected thresholds. 

    The output of the application of each ICP iteration is a single
    transformation matrix \textbf{T} that minimizes the point-to-plane error
    metric, defined as as the sum of squared distances between each point in the
    the frame and the tangent plane at its corresponding point in the previous
    frame:
    \begin{equation}
      arg\text{ }min \sum\limits_{u \atop D_i(u) > 0} ||Tv_i(u) - v_{i -
      1}^{g}(u).n_{i - 1}{g}(u)||^{2}
    \end{equation}

    A linear approximation is created that solves this system only for the case
    where an incremental transformation occurred (for more details, see
    \cite{kinectfusion}). 

    Using this, implemented for the GPU, it is possible to perform ICP on all
    the measurement provided in each 640x480 depth map and to segment the
    foreground from the background. The last think is one of the big
    contributions of the article.
  % subsection Camera Tracking (end)

  \subsection{Volumetric Representation} % (fold)
  \label{sub:Volumetric Representation}
    By obtaining the position of the camera, the depth measurements can be
    converted to global coordinates and integrated in the representation of the
    geometry of the scene. For that representation the authors have used
    volumetric surface representation, based on
    \cite{representaion-and-rendering-of-implicit-surfaces}.

    Unlike it is proposed in the article
    \cite{representaion-and-rendering-of-implicit-surfaces}, however, the
    authors of \cite{kinectfusion} have decided not to implements octree, but
    instead to use a uniformly subdivided space into voxels (voxels are
    volume elements in regular grid in 3D, something similar to pixels in
    pictures; for more information, see \cite{voxel}). The global 3D vertices
    that are part of the scene are integrated into voxels using a varian of
    Singed Distance Functions (SDFs), that specify the signed distance between a
    point and the surface. The values of those functions are positive for points
    in front of the surface, negative for points behind the surface, and the
    surface is depicted as the place where the values change sign. 

    As it is practically impossible to store all the values, it is widely used
    to keep just a truncated region around the actual surface. The so described
    functions are refered to as Truncated Signed Distance Functions (TSDFs).
    This representation makes integration of new data quite easy and efficient
    and in the same time implicitly encodes the uncertainty in the range data.
    This makes it more appropriate then meshes, for example.
  % subsection Volumetric Representation (end)
% section Implementation (end)

\begin{thebibliography}{widest entry}
  \bibitem{kinectfusion} Shahram Izadi et al., KinectFusion: Real-time 3D
    Reconstruction and Interaction Using a Moving Depth Camera, 2011
  \bibitem{representaion-and-rendering-of-implicit-surfaces} Christian Sigg, et
    al., Representation and Rendering of Implicit Surfaces, 2006
  \bibitem{icp} P. J. Besl and N. D. McKay. A method for registration of 3D
    shapes. IEEE Trans. Pattern Anal. Mach. Intell., 14:239–256, February 1992. 
  \bibitem{body-scanning} Jing Tong et al., Scanning 3D Full Human Bodies using
    Kinects, 2012
  \bibitem{efficient-icp} S. Rusinkiewicz and M. Levoy. Efficient variants of the
    ICP algorithm. 3D Digital Imaging and Modeling, Int.
    Conf. on, 0:145, 2001.
  \bibitem{voxel} http://en.wikipedia.org/wiki/Voxel
\end{thebibliography}

\hfill\\

\end{document}
