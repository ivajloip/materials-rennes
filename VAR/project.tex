\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}

\oddsidemargin 0mm
\evensidemargin 0mm
\topmargin 0mm
\textheight 210mm
\textwidth 160mm

\pagestyle{fancy}
\fancyhf{}
\fancyhead[CE,CO]{VAR paper evaluation by Ivaylo Petrov}
\fancyfoot[CE,CO]{\thepage}

\lstset{basicstyle=\ttfamily,
  mathescape=true,
  escapeinside=||,
  emph={Task, Operation, for, each, if, else, endif, endfor, while, then, When,
  do, repeat, until, endrepeat},
  emphstyle={\color{gray}\bfseries\itshape}}

\setlist[itemize]{noitemsep,nolistsep}
\setlist[enumerate]{noitemsep,nolistsep}

\def\CC {{\mathbb C}}        
\def\RR {{\mathbb R}}       
\def\ZZ {{\mathbb Z}}      
\def\NN {{\mathbb N}}     
\def\be  {\begin{eqnarray}} 
\def\ee  {\end{eqnarray}}  
\def\ben {\begin{eqnarray*}} 
\def\een {\end{eqnarray*}}   
\newcommand{\hr}{\rule{\linewidth}{0.1mm}}
\newcommand{\bighs}{\hspace{15pt}}
\newcommand{\hs}{\hspace{10pt}}
\renewcommand{\tilde}{\overset{-}}

\newenvironment{itemize*}{
  \begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
}{
  \end{itemize}
}

\newenvironment{enumerate*}{
  \begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
}{
  \end{enumerate}
}

\newtheoremstyle{plain}{1pt}{0pt}{}{}{}{}{.5em}{\thmname{\textbf{#1}}:\thmnote{#3}}

\theoremstyle{plain}

\everymath{\displaystyle}

\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}}

\begin{document}

\bibliographystyle{plain}

\section*{\centering
  Paper evaluation of\\
  \emph{"KinectFusion: Real-time 3D Reconstruction and Interaction Using a
    Moving Depth Camera"}\\
  by\\
  Ivaylo Petrov
}

\hr

The following article is going to provide information about \emph{Real-time 3D
Reconstruction and Interaction Using a Moving Depth Camera}. The main focus 
will be on the paper \emph{"KinectFusion: Real-time 3D Reconstruction and
Interaction Using a Moving Depth Camera"} by Shahram Izadi, David Kim,
Otmar Hilliges, David Molyneaux, Richard Newcombe, Pushmeet Kohli,
Jamie Shotton, Steve Hodges, Dustin Freeman, Andrew Davison, Andrew Fitzgibbon.

\section{Abstract} % (fold)
\label{sec:Abstract}
  Creation of an relatively accurate 3D map of some objects with a low-cost 
  hand-held device has been a goal of a number of researches. Now with a Kinect
  camera and appropriate software that is possible. Furthermore, with this 
  camera now we are not only able to rapidly create a detailed 3D reconstruction
  of a limited size rigid scene (a room for example) in real time
  \cite{kinectfusion}, but we can also use this reconstruction to create
  geometry-aware augmented reality with physical interactions within it. 

  KinectFusion is a software that is capable of using a Kinect camera to do all
  that without the need of additional widely used devices such as trackers. It
  also provides us with the possibility to enable real-time multi-touch
  interactions anywhere, allowing any planar or non-planar reconstructed
  physical surface to be appropriated for touch.
% section Abstract (end)

\section{Introduction} % (fold)
\label{sec:Introduction}
  For quite some time people have been fascinated by the possibility with an 
  affordable, hand-held device, to scan accurately 3D shapes in real time,  
  to create geometry-aware augmented reality, and to create physically based
  interactions within it. So far, however, this has been impossible. With the
  inventing of the Kinect camera the world is presented with the possibility to
  create relatively accurate 3D maps for very low price. For that reason this
  device rapidly gained popularity among researchers and enthusiasts alike.

  While this device does not provide a conceptually new approach, structured
  light technique to obtain depth map is not a new idea, its low-cost and
  relative accuracy contribute greatly to its success. As the authors of the
  article note, however, the noise that is caused by fluctuations and holes
  where reads were not obtained need to be taken care of with some algorithms in
  order to make the device really useful for the above scenarios. 

  The already described applications of this device, with the appropriate
  software, range from gaming and physics to CAD programs. For all those things,
  however, an accurate model should be inferred from the noisy point-based data.
  To do that the authors consider different approaches and finally decide to use 
  \textbf{volumetric surface representation}. Although this approach has some 
  drawbacks, i.e. hight memory consumption, which might potentially lead to high
  computational cost (as noted in
  \cite{representaion-and-rendering-of-implicit-surfaces}). There is a
  solution to this problem, that the authors suggest - the use of octree.
  Although there is no additional information about this approach in the
  original article, such will be provided with some additional details. 

  As the system is designed in a way that allows moving of the Kinect camera and
  interaction of users with the scene, the authors needed to solve a number of
  difficulties that those two characteristics introduced. In order to track the
  camera position, they have decided to track the \emph{6 degrees-of-freedom
  (DOF)}. They divide the input in two parts - foreground and background and use
  only the data from the background to calculate the 6DOF, because as algorithms
  generally depend on the staticness of the scene, it is impossible to
  accurately calculate the camera position if all the data is considered. This
  allows complex user interactions with the camera that does not significantly
  degrade the quality of the camera tracking. This division of foreground and
  background objects makes possible the use of the system to detect multi-touch
  interactions with physical surfaces of any type - planar or not.

% section Introduction (end)

\section{Related work} % (fold)
\label{sec:Related work}
  Although scene geometry reconstruction using various techniques such as active
  sensors, passive cameras, online images or from unordered 3D points, is well
  studied in the field of computer graphics and vision, the possibility of
  interaction with the scene is what distinguishes \cite{kinectfusion} from all
  proposed solutions until now. The area of Simultaneous Localization and
  Mapping (SLAM) is also tightly connected with the proposed system, as it has
  to determine its position on in real time as new observations are provided,
  while at the same time create a precise 3D representation of the observed
  objects and cope with user interactions. How this problem is solved in the
  described article will be explained later.

  Other differences between KinectFusion and previous works can be described as:
  \begin{itemize}
    \item \textbf{Interactive rates} In contrast with most of the already
      existing solutions, KinectFusion wants to provide real-time computation of
      both camera position tracking and world geometry reconstruction. This is
      essential for some of the applications that are targeted by the authors
      and most of all for games and augmented reality, where direct feedback and
      user interactions are key to the usability of the system. By the time that
      the article was published, there was no system, to the knowledge of the
      authors, that provided interactive rates for those two things.
    \item \textbf{No explicit feature detection} Unlike some of the previously
      created techniques that use Structure from Motion (SfM) or RGB plus depth
      (RGBD) detect sparse scene features, the proposed solution uses directly
      in some way all the data acquired by the Kinect sensor. This also makes
      the system independent of the light conditions in the observed scene.
    \item \textbf{High-quality geometry reconstruction} The authors of the
      system, unlike many other before them, do not want to sacrifice details in
      the reconstructed model in order to obtain real time tracking. Often,
      sparse maps are used for the localization, but they can not be used as a
      mean to reconstruct the observed geometry. Instead of that, the authors
      use volumetric surface representation, which enables them to directly
      reconstruct surfaces that can also be used for rendering of the scene,
      possibly with objects from augmented reality, in real time.
    \item \textbf{User interaction assumed} In most of the previously provided
      studies, the scene was expected to be completely static in order for the
      systems to work. The authors of \cite{kinectfusion} created a system that
      along with tracking the camera and reconstructing the scene, allow user
      interaction. This requires the selection of an representation, that can
      easily deal with this dynamicity, without compromising the real-time
      nature of the computations.

      It is also mentioned that systems that use ToF, which stands for
      Time-of-Flight does not deal with dynamic scenes. This is peculiar, as the
      technology of ToF is just used to obtain depth map. There are two ways
      those maps to be obtained - through devices that use ToF, which however is
      currently too pricey, or projecting a known infrared pattern onto the
      scene and determining depth based on the pattern's deformation
      \cite{body-scanning}. The later is, however, much cheaper. With this said,
      it is not clear why the authors of \cite{kinectfusion} mix depth map
      observation with dynamic scene. There might have been a reason for that,
      but it should have been better described.
    \item \textbf{Mobility without additional infrastructure} The idea of the
      project is to create mobile (at least to some degree) system that can be
      used to reconstruct any indoor spaces. This distinguishes the proposed
      solution from many earlier works that used fixed or large sensors, or
      sensors that are fully embedded in the environment. Also in order the
      application to be easier to use, the authors wanted to avoid using
      augmentation of the space and the interacting entities with some markers.
    \item \textbf{Size scale} Additional requirement that distinguishes the
      proposed approach is the support of bigger scenes - a whole room for
      example. In the past most of the detailed reconstruction techniques that
      were proposed would be usable only for relatively small geometries.
    \item \textbf{Low-cost scanning} The combination of low-cost, real-time and
      excellent geometry details has not been shown by any of the previous
      works. Those things contribute to much better user experience, as the user
      can instantly verify that the scanned object has the necessary properties
      and proceed with the use of the created model. Furthermore, the system can
      be used not only to scan static scenes, but also to scan objects that are
      rotated by the user in front of the camera. For the latter, however, there
      are a few requirements that should be satisfied in order this use to be
      applicable. First of all, the object should occupy a sufficiently large
      portion of the depth map in order ICP to be able to converge correctly.
      The moving of the object should not be too quick, as this might cause
      problems to the tracking algorithm as well.

      It is still possible to scan smaller object, provided that first the
      system is able to reconstruct sufficiently accurate representation of the
      surrounding world and just after that the object is presented for
      scanning.
  \end{itemize}
% section Related work (end)

\section{KinectFusion} % (fold)
\label{sec:KinectFusion}
  Following the example of the authors of \cite{kinectfusion}, we will now
  present a number of applications of the KinectFusion system and after that
  some of its parts will be described in more details and additional information
  will be provided from \cite{representaion-and-rendering-of-implicit-surfaces}
  and \cite{icp}.

  The system tries to constantly keep track of the camera position in the space,
  calculating constantly the 6DOF and adding new data as camera is moved or new
  observations are added for places, where before that there were holes, i.e.
  there was no information for that part of the scene. The camera motion does
  not have to be too big in order new data to be acquired, this can happen, for
  example, because of a slight camera shake. The authors point out that given
  the real-time speed of the reconstruction, the level of details of the
  reconstructed objects is more than satisfactory. More details on that will be
  provided later in this paper. Using the RGB camera that the Kinect sensor has,
  it is also possible to map textures to the reconstructed objects as shown in
  Figure~\ref{fig:model_with_texture}B.

  \begin{figure}
    \centering
    \includegraphics[height=40mm]{graphics/augented_reality.png}
    \includegraphics[height=40mm]{graphics/model_with_texture.png}
    \includegraphics[height=40mm]{graphics/augmented_reality_with_a_number_of_small_spheres.png}
    \caption{Model with texture mapped from Kinect RGB camera}
    \label{fig:model_with_texture}
  \end{figure}

  One of the possible application of the system is for \textbf{creation of
  augmented reality}. It is possible to add some 3D objects in the model,
  created by the system and then during the rendering to show them to the user
  as viewed from the position of the kinect camera, reflecting or casting
  shadows to neighboring objects. The requirement to view the scene from the
  kinect camera position comes from the fact that for texture mapping, the
  system uses the build-in Kinect RGB camera, which is at the same position as
  the depth camera.  In Figure~\ref{fig:model_with_texture}A an additional metal
  sphere can be seen, that is not presented in the physical scene, but that
  looks exactly as if it was at that position during the taking of the picture.

  Another application is \textbf{physical world dynamics simulation}. That means
  that thanks to a special module of the system, it is possible to create
  virtual objects that are dynamic, meaning that they change their place and
  also to simulate rigid body collisions between real and virtual objects that
  are physically precise and, of course, real-time. This can have great
  application in areas such as gaming and robotics. The number of virtual
  objects is not precisely limited, it is believed that as much as a few
  thousands of them can be simultaneously introduced in the scene, without
  breaking the time bounds of the execution. Such scene can be viewed in
  Figure~\ref{fig:model_with_texture}C, where there is a big number of small
  spheres that are added to the real scene.

  As the authors of the article explain, it proves to be very useful to divide
  the scene in two parts - foreground and background. In the foreground is the
  interacting and moving user and in the background is the static scene. This
  segmentation is achieved through declaration of the pixels where the scene has
  changed too much as outliers that are after that processed and separately
  rendered and added in the final visualization. 

  It is this segmentation that allows relatively rapidly moving foreground
  objects to interact and collide with rigid virtual ones. 

  What is maybe even more interesting about this segmentation is that it
  actually allows us to calculate interactions between foreground objects
  (users) and background scene. The points of interaction indicate the positions
  where the user comes into contact with the scene. This can be used to detect
  touching points between the user and any surface, which in turn can be used to
  allow direct multi-touch interactions (see Figure~\ref{fig:interactions}A and
  B).

  \begin{figure}
    \centering
    \includegraphics[height=40mm]{graphics/interaction_actual.png}
    \includegraphics[height=40mm]{graphics/virtual_interactoin.png}
    \caption{Interactions between user and environment}
    \label{fig:interactions}
  \end{figure}
% section KinectFusion (end)

\section{Implementation} % (fold)
\label{sec:Implementation}
  The implementation of the system has to rely on significant computational
  power for a great number of relatively simple tasks. For that purpose the
  authors decided to use GPUs as they are specifically designed for such
  applications and as there are possibilities to use them not only to do
  graphical computations, but also to do a wide range of concurrent
  computations, that are relatively independent as is the currently depicted
  case. For the implementation the authors of the article use CUDA (a parallel
  computing platform and programming model created by NVIDIA and implemented by
  the graphics processing units (GPUs) that they produce) in order to run a big
  number of parallel threads simultaneously. 

  The computational process that is presented can be divided in the following
  parts that will be described after that: Depth Map Conversion, Camera
  Tracking, Volumetric Integration, Raycasting (see Figure~\ref{fig:pipeline}).
  Each of them is executed for each new data provided by the sensor on the GPU
  unit.

  \begin{figure}
    \centering
    \includegraphics[height=55mm]{graphics/pipeline.png}
    \caption{Computational pipeline}
    \label{fig:pipeline}
  \end{figure}

  \subsection{Depth Map Conversion} % (fold)
  \label{sub:Depth Map Conversion}
    It is possible to imagine that each observation \emph{i} that we have
    represent the state of the scene at some time $i * m$ where m is the time
    between two consecutive observations. If we denote the depth map as
    $D_i(u)$, where $u = (x, y)$ is a pixel obtained by the infrared camera and
    the calibration matrix of the Kinect infrared camera as K, we can compute
    the depth measurements as a 3D vertex in the camera's coordinate space as
    follows: $v_i(u) = D_i(u) K ^ {-1}[u, 1]$. This can be computed in parallel
    on the GPU. After that normal vectors for each vertex can be computed, again
    in parallel by the GPU, using the neighboring vertexes (see
    \cite{kinectfusion} for details).
    Given that the 6DOF are presented, i.e. a matrix $T_i = [R_i|t_i]$ that
    contains 3x3 rotation matrix($R_i$) and 3D translation vector ($t_i$), it is
    possible to convert the vertexes and the normals from the coordinate space
    of the camera to some global coordinate space.
  % subsection Depth Map Conversion (end)

  \subsection{Camera Tracking} % (fold)
  \label{sub:Camera Tracking}
    In order to track the camera pose for each new depth frame, the authors have
    decided to use the well studied and very popular ICP algorithm (see
    \cite{icp} and \cite{efficient-icp} for more details). Typically this
    algorithm is used for alignment of 3D shapes, but in this case it is used to
    find 6DOF transformation that most closely aligns the current oriented
    points with those of the previous frame. The authors use \emph{projective
    data association} to do this. Those transformations can be incrementally
    applied to give the single global camera position $T_i$.

    While doing this, the algorithm also detects potential outliers by
    calculating distances and angles in the global coordinate system and
    checking that they are within some expected thresholds. 

    The output of the application of each ICP iteration is a single
    transformation matrix \textbf{T} that minimizes the point-to-plane error
    metric, defined as as the sum of squared distances between each point in the
    the frame and the tangent plane at its corresponding point in the previous
    frame:
    \begin{equation}
      arg\text{ }min \sum\limits_{u \atop D_i(u) > 0} ||Tv_i(u) - v_{i -
      1}^{g}(u).n_{i - 1}{g}(u)||^{2}
    \end{equation}

    A linear approximation is created that solves this system only for the case
    where an incremental transformation occurred (for more details, see
    \cite{kinectfusion}). 

    Using this, implemented for the GPU, it is possible to perform ICP on all
    the measurement provided in each 640x480 depth map and to segment the
    foreground from the background. The last think is one of the big
    contributions of the article.
  % subsection Camera Tracking (end)

  \subsection{Volumetric Representation} % (fold)
  \label{sub:Volumetric Representation}
    By obtaining the position of the camera, the depth measurements can be
    converted to global coordinates and integrated in the representation of the
    geometry of the scene. For that representation the authors have used
    volumetric surface representation, based on
    \cite{representaion-and-rendering-of-implicit-surfaces}.

    Unlike the original proposition in article
    \cite{representaion-and-rendering-of-implicit-surfaces}, however, the
    authors of \cite{kinectfusion} have decided not to implements octree, but
    instead to use a uniformly subdivided space into voxels (voxels are
    volume elements in regular grid in 3D, something similar to pixels in
    pictures; for more information, see \cite{voxel}). The global 3D vertices
    that are part of the scene are integrated into voxels using a varian of
    Singed Distance Functions (SDFs), that specify the signed distance between a
    point and the surface. The values of those functions are positive for points
    in front of the surface, negative for points behind the surface, and the
    surface is depicted as the place where the values change sign. 

    As it is practically impossible to store all the values, it is widely used
    to keep just a truncated region around the actual surface. The so described
    functions are refered to as Truncated Signed Distance Functions (TSDFs).
    This representation makes integration of new data quite easy and efficient
    and in the same time implicitly encodes the uncertainty in the range data.
    This makes it more appropriate then meshes, for example.

    The authors seem to consider the approach where the space is uniformly
    divided into voxels as better both in terms of easiness of use and
    computational efficiency then octree approach. There are no evidence to
    support this, however. It is clear, though, that this approach does not
    scale good with the increase of the size of the scene. It does not matter
    what is in the scene, only its size. This is not true for the representation
    using octrees that essentially tries to store as little information as
    possible in order to ensure preservation of the details.

    From one point of view it is normal that the uniform space division was
    chosen by the authors as they want to scale just as much as to be able to
    scan a whole room.  Another reason why they would not care about that is
    that the Kinect infrared camera starts to loose accuracy after some distance
    (after 2 or 3 meters the values are significantly noisy). This was not
    mentioned by the authors as maybe they have considered it irrelevant in the
    case that they considered. However for the purposes of \cite{body-scanning}
    this is quite important. For their purpose it is essential that great
    details can be obtained by Kinect infrared camera only from small distance.

    As for the computational performance, although true that the uniform space
    division structure is quite easy to run in parallel, it is possible that a
    great majority of the voxels does not contain data and are processed for no
    reason. In such situations maybe an octree would provide better results. It
    would be interesting to take experimental results for both in different
    scenes and see how they perform in them - how the number of surfaces will
    impact the performance.

    The authors admit that their approach with the uniformly divided space is
    far from being memory efficient. The better representation (at least in some
    sense) would not only reduce the memory footprint, but it will also make
    possible extra details to be added to some part of the scene. With the
    uniform representation it is not possible if we are specifically interested
    in some part of the scene to get extra details for it. This is natural for
    the octree representation. Maybe a reconsideration of the volumetric surface
    representation can improve the provided by the authors of
    \cite{kinectfusion} results.

    The way that the TSDF is calculated is the following: a metric distance from
    the camera center to each vertex can be calculated. This 3D vertex is
    perspective projected back into the image coordinates to lookup the actual
    depth measurement along the ray. The difference between the measured and the
    calculated distances gives a new SDF value for the voxel. This value is
    normalized and averaged with some weight with the previously stored value. 
  % subsection Volumetric Representation (end)

  \subsection{Raycasting for Rendering and Tracking} % (fold)
  \label{sub:Raycasting for Rendering and Tracking}
    The authors use a GPU-based raycaster to generate views of the modeled scene
    within the volume. Separate thread is walking a single ray and renders a
    single pixel in the output image. With starting point and a direction, each
    thread traverses voxels along the ray and search for a point of
    zero-crossing. After that some interpolation is used to precisely determine
    the point of the surface. Using the TSDF the normal is computed as a
    derivative at the zero-crossing. This provides a single interpolated vertex
    and normal, which are then used for lighting calculations.

    It is not clear how exactly objects added by the augmented reality layer of
    the system get accurate lighting calculations, as the sources of the light
    are not implicit and as a consequence if the artificially added objects are
    supposed to reflect some light from the neighbouring objects, it is not
    possible with the single Kinect RGB camera to calculate the correct colors.
    This slightly reduces the number of possible realistically appearing objects
    that can be added to the scene, but this is generally not that important.
    Furthermore, it might be possible to overcome this problem, however the
    authors have not provided information as to how this can be accomplished.
  % subsection Raycasting for Rendering and Tracking (end)

  \subsection{Simulating Real-World Physics} % (fold)
  \label{sub:Simulating Real-World Physics}
    As it was already pointed out in the first part of this paper, the proposed
    system KinectFusion can support simulations of physically realistic
    collisions between virtual objects and the reconstructed scene. This part of
    the system also uses the GPU and as a consequence real-time computations are
    still possible even with a great number of virtual objects. Here is how it
    is done: during the rendering of the scene, TSDF values within some adaptive
    threshold are extracted. For each surface voxel, a static particle is
    instantiated that contains: a 3D vertex in global coordinates, a velocity
    vector that represent the direction and the speed of the particle (always
    empty for static particles) and an ID. Having this representation, it is
    possible to observe the dynamicity of the system, but additional steps
    should be taken in order to actually detect the collisions. The authors use
    a uniform grid to identify neighbouring cells. They first assign each
    particle to a grid cell. After that a separate thread is run for each
    dynamic particle to detect if it collides with some of its neighbouring
    objects. The interaction with each of them is taken into account and this
    determines the new direction and speed of the particle (and consequently its
    new position in the next frame). 

    While this approach seem to work nice according to the authors of
    \cite{kinectfusion}, the uniform subdivision of the space seem as not the
    best possible solution. It is possible that the majority of the particles
    are concentrated in some part of the space and thus are neighbours in the
    neighbouring grid. This will lead to significant reduction of the speed of
    he system, especially when the objects to be tested for collisions are not
    spheres, because for a number of objects the test for collision between two
    objects is much slower then for spheres. Maybe a better approach would be
    the use of a better space dividing technique, for example k-d trees or
    octrees. With them it is possible to take into account the locality of the
    elements, while at the same time keeping the number of particles in each
    volume below some threshold (see \cite{fast-collision-detection} and
    \cite{collision-detection-based-on-partitioning} for more details). Note,
    however that there is always a trade-off between easiness of collision
    detection and easiness of maintenance of the data structure.
  % subsection Simulating Real-World Physics (end)

  \subsection{User Interaction} % (fold)
  \label{sub:User Interaction}
    As previously was stated, one of the goals of the project is to allow user
    interaction with the scenes. This creates two main problem with the so far
    described approach. First of all, the camera tracking algorithm relies on
    the fact that between two frames there has been only one rigid
    transformation. With user interacting independently of the camera motion
    this assumption is no longer true. As already stated, sufficiently big
    change in the scene may cause the ICP algorithm to fail to converge.

    The other major problem is that so far every observation was integrated in
    the TSDF function with some weight that shows how readily the new value
    should be accepted as correct. Smaller value will cause slower update of the
    surface, whereas big value might integrate too much of the noise in the
    surface. As a consequence of the user interaction, however, regardless of
    the weight value, some of the data from the moving user will be integrated
    in the scene model, which is not desirable. This could also make impossible
    correct camera tracking.
  % subsection User Interaction (end)

  \subsection{ICP Outliers for Segmentation} % (fold)
  \label{sub:ICP Outliers for Segmentation}
    Maybe one of the greatest contributions of this article is the proposition
    of a way to segment foreground moving items from background ones. The idea
    is that if initially the sensor can obtain some approximate representation
    of the scene and after that when the user comes and starts to interact, it
    is possible to detect the points that are significantly different from the
    expected values. Those points are marked as outliers. Later, this is the
    important thing, they assume that any interacting body will represent a
    significant surface. This is to say that there will be a number of
    neighbouring outliers. As a consequence, it can be concluded that those
    measurements are not errors of the sensor, but instead are moving objects.
    Having isolated them, we can substitute their values with values from the
    model in the observation, so that those new depth measurements does not
    distort the model.  The foreground objects on the other hand can be handled
    separately. For them, however, we know the precise global position of
    the camera, and so it is possible to regard them as the case when an object
    is presented in from of the static camera for scanning, i.e. to run a
    separate ICP for them and integrate the new data in a separate model. It
    turns out that even if some small part of the foreground model is moving
    non-rigidly, the ICP algorithm still manages to converge on the other,
    rigidly moving parts. That way the foreground objects can safely be
    extracted from the outliers from the depth map and integrated into a second
    model. After the geometries of the two are determined, we can render them
    separately and then combine the two results.

    According to the authors of \cite{kinectfusion} this technique gave very
    good results for all their tests. It is not clear, however, if the initial
    scene is at sufficient distance from the Kinect infrared camera how precise
    will be representation and if this can cause failure to detect background
    object from foreground ones (interacting users). For example, the foot of
    the user is expected to take just a small portion of space and maybe at
    appropriate distance (3-4 meters) it would be impossible to distinguish it
    from the background. This of course is some limitation of the camera, but
    maybe it is worth noticing.

    As for the physical simulation, the authors of \cite{kinectfusion} have
    decided to represent the entire foreground reconstruction as static
    particles. This way they can do the same as for the background part. It
    would be more interesting if they would try to determine the dynamicity of
    the foreground object as this way it would be possible to interact more
    precisely with virtual objects - for example hit a virtual bow with your
    hand and observe it moving with velocity dependent on the velocity of the
    hand.
  % subsection ICP Outliers for Segmentation (end)

  \subsection{Touch detectoin} % (fold)
  \label{sub:Touch detectoin}
    When the two surfaces are represented in the virtual space, it is possible
    to try to find some positions where they are very near to one another and
    declare them as touch points. This is done by extending slightly the
    raycasting of the background (for more details see \cite{kinectfusion}).
  % subsection Touch detectoin (end)
% section Implementation (end)

\section{Comparison between KinectFusion and a technique for Full human body
scanning} % (fold)
\label{sec:Comparison between KinectFusion and a technique for Full human body
scanning}
  Although the results from \cite{kinectfusion} are really good, the system
  could not be used to create a sufficiently detailed human body model (see
  \cite{body-scanning}). One of the difficulties with the scan of the whole
  human body is that either the camera should be at sufficiently big distance
  from the person that is to be scanned, which leads to results similar to those
  of Figure~\ref{fig:human-body}(a), or it should be moved along the body from
  smaller distance. The problem that still remains with the second approach is
  that the human body is non-rigid and special techniques should be used to
  overcome this complication. Two major problems are registration of non-rigid
  objects and global aliment of non-rigid objects, i.e. the loop closure
  problem. Solutions for both are provided by \cite{body-scanning}.

  \begin{figure}[h]
    \centering
    \includegraphics[height=80mm]{graphics/human_body.png}
    \caption{(a) The raw data of capturing a full human body with one single
      Kinect has much low quality as the Kinect has to be put far from the
      body. (b) The raw data captured using the system, which is proposed by the
      authors of \cite{body-scanning} has higher quality as multiple Kinects are
      used to capture different parts of the body at a closer distance. (c) The
      reconstructed human model created using their system.}
    \label{fig:human-body}
  \end{figure}

  Let us just note that the fact that KinectFusion can not provide good results
  in this situation is to be expected. The authors specifically say that it has
  problems with non-rigid bodies. Still, it is good to see how big are the
  problems in some situations and that there are other more specific solutions
  for those other situations.
% section Comparison between KinectFusion and a technique for Full human body
% scanning (end)

\section{Conclution} % (fold)
\label{sec:Conclution}
  As a conclusion, it can be said that the main article that was depicted, i.e.
  \cite{kinectfusion} is a very good one. It provides information for a system
  that can be used in a number of real life applications, ranging from creation
  of 3D models with low-cost equipment, to playing video games and interacting
  with augmented reality. There are of course things that can explained in more
  details or things that can be tested to see if they will work better. Maybe
  the uniform division of space will not be the best solution for the surfaces
  and for the collision detection. There are not sufficient details maybe for
  the Kinect infrared camera, but most likely the authors have considered it
  popular enough so that they do not have to provide many technical
  characteristics. 
  Apart from all those details, the article can be said to have provided a
  number of new ideas to the field and thanks to that, I think that it is a
  really good one.
% section Conclution (end)

\begin{thebibliography}{widest entry}
  \bibitem{kinectfusion} Shahram Izadi et al., KinectFusion: Real-time 3D
    Reconstruction and Interaction Using a Moving Depth Camera, 2011
  \bibitem{representaion-and-rendering-of-implicit-surfaces} Christian Sigg, et
    al., Representation and Rendering of Implicit Surfaces, 2006
  \bibitem{icp} P. J. Besl and N. D. McKay. A method for registration of 3D
    shapes. IEEE Trans. Pattern Anal. Mach. Intell., 14:239–256, February 1992. 
  \bibitem{body-scanning} Jing Tong et al., Scanning 3D Full Human Bodies using
    Kinects, 2012
  \bibitem{efficient-icp} S. Rusinkiewicz and M. Levoy. Efficient variants of the
    ICP algorithm. 3D Digital Imaging and Modeling, Int.
    Conf. on, 0:145, 2001.
  \bibitem{voxel} http://en.wikipedia.org/wiki/Voxel
  \bibitem{fast-collision-detection} S. Bandi, D. Thalmann, "An adaptive spatial
    subdivision of the object space for fast collision detection of animating
    rigid bodies", Proc. of Eurographics, pp. 259-270, 1995
  \bibitem{collision-detection-based-on-partitioning} Matthias Teschner, course
    materials for "Image Processing and Computer Graphics", part "Collision
    Detection based on Spatial Partitioning", University of Freiburg 
\end{thebibliography}

%\hfill\\
\end{document}
