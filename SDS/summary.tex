\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english,bulgarian]{babel}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{listings}
\usepackage{xcolor}

\oddsidemargin 0mm
\evensidemargin 0mm
\topmargin 0mm
\textheight 216mm
\textwidth 165mm

\pagestyle{fancy}
\fancyhf{}
\fancyhead[CE,CO]{Summary of \emph{"Map-Based Graph Analysis on MapReduce"}}
\fancyfoot[CE,CO]{\thepage}

\lstset{basicstyle=\ttfamily,
  mathescape=true,
  escapeinside=||,
  emph={Task, Operation, for, each, if, else, endif, endfor, while, then, When, do, repeat, until, endrepeat},
  emphstyle={\color{gray}\bfseries\itshape}}

\def\CC {{\mathbb C}}        % Complex numbers
\def\RR {{\mathbb R}}        % Real numbers
\def\ZZ {{\mathbb Z}}        % Integers
\def\NN {{\mathbb N}}        % Positive integers
\def\be  {\begin{eqnarray}}  % Begin of math formula with number
\def\ee  {\end{eqnarray}}    % End of math formula with number
\def\ben {\begin{eqnarray*}} % Begin of math formula without number 
\def\een {\end{eqnarray*}}   % End of math formula with number 
\newcommand{\hr}{\rule{\linewidth}{0.1mm}}
\newcommand{\bighs}{\hspace{15pt}}
\newcommand{\hs}{\hspace{10pt}}
\renewcommand{\tilde}{\overset{-}}

\newenvironment{itemize*}{
  \begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
}{
  \end{itemize}
}

\newenvironment{enumerate*}{
  \begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
}{
  \end{enumerate}
}

\newtheoremstyle{plain}{1pt}{0pt}{}{}{}{}{.5em}{\thmname{\textbf{#1}}:\thmnote{#3}}

\theoremstyle{plain}
\renewcommand\thesection{\Roman{section}.}
\renewcommand\thesubsection{\thesection\Roman{subsection}.}
\renewcommand\thesubsubsection{\thesubsection\Roman{subsubsection}.}

% Makes the size of the symbols in the formulas fixed (not getting smaller)
\everymath{\displaystyle}

\begin{document}

\section*{\centering
  Map-Based Graph Analysis on MapReduce - summary \\
  Ivaylo Petrov 
}

\hr

\section{Paper Content} % (fold)
\label{sec:Paper Content}
The article that is going to be assessed is \emph{Map-Based Graph Analysis on
MapReduce} by Upa Gupta and Leonidas Fegaras. It provides information about 
data processing using MapReduce framework and more specifically big graph 
processing that requires iterative graph algorithms. The authors of the article
believe that there is a need for efficient design pattern for solving such
problems, as more and more graph data is generated and the size of those
graphs is growing. They propose such a design pattern based on separation of
the immutable graph topology from the graph analysis results.

\subsection{Introduction} % (fold)
\label{sub:Introduction}
  The introduction of the article first present the importance of the problem - 
  as more and more companies need to process ever increasing amount of data,
  that is presented in the form of graphs (usually millions of nodes and
  billions of edges), it is often required to use in a way the de-facto
  framework for large-scale data analysis $\rightarrow$ Hadoop.

  The authors also point out, that in most cases the element that has the
  greatest impact on the performance of the algorithms that use Hadoop, is the
  communication time. Although there have been previous attempts to improve
  this aspects of the algorithms (introducing some new design patters for
  example), there is still possibility for improvement.

  The basic idea of the authors is to introduce a new design pattern for a 
  family of iterative graph algorithms. They use an example to describe their
  idea and to illustrate how it can be used in practice. The example is the
  page-ranking algorithm. Although it is described in details how this algorithm
  can be made to use their design pattern, the pattern that they propose can be
  used for many other problem. As expected the improvement has its price and 
  some restrictions for cases where it can be applied. The restriction of the
  proposed design pattern is that it only works for \emph{"iterative graph algorithms
  on directed graphs where partial results associated with nodes can be
  improved at each iteration"}[1]. In other words only data about the notes is
  computed and stored and the algorithm should be trying to optimize it on each
  iteration. There are many situations where such algorithms are needed and as
  a consequence the possible improvement is very important.
  
  Although the proposed algorithm is presented in more details later, a rough
  explanation will be given, in order the reader to attain a better
  understanding of the explanations that are to follow.

  The idea of the algorithm is to divide the graph in several parts (a multiple
  of the number of computational nodes in most cases). Every part is processed
  by a single node that is always the same, i.e. it does not change during the
  different iterations. This way some data that is required for the computations
  (the graph topology that the node should process) can be made local for the
  node and the cost of the communication can be decreased. As the computation of
  each node are dependent on the computations of all the nodes during the previous 
  iteration, all the result from the previous iterations are written on the 
  distributed file system and thus every node can access it for its needs. This
  way the need of reducers is eliminated and as a consequence, it is possible to
  decrease the communication (as shuffling of data is skipped).

  On the overall, the introduction gives good idea about the article.
% subsection Introduction (end)

\subsection{Related work} % (fold)
\label{sub:Related work}
  In this part of the article we are briefly introduced to Twister and Haloop,
  which are attempts to improve the inefficiency of MapReduce framework for 
  graph processing. The two however cannot scale which means that for most 
  contexts they are not usable.

  We are also introduced to Schimmy design pattern[2], but as a disadvantage of
  this technique, the authors of the article point out the fact that it
  "requires the shuffling of the partial results"[1].

  After that it is explained that Incoop[3] and Google's Percolator[4] might be
  good for some purposes, but as they do not target general graph processing, 
  they can not be compared to the proposed design pattern.

  Finally, the bulk synchronous parallel (BSP) paradigm is introduced. As "it
  requires that the whole graph be stored in the collective memory of the
  cluster"[1], which greatly limits the size of the graph that is to be analyzed
  and so this technique is also not better than the proposed by the article.

  On the overall, the related work section provides a number of examples of 
  other attempts to solve the same or similar problem and for each of them it
  states what is the advantage of the proposed by the article solution.
% subsection Related work (end)

\subsection{MapReduce} % (fold)
\label{sub:MapReduce}
  In this part of the article, the authors describe the MapReduce framework -
  how it works, when some actions are executed and other details. For more
  details, please refer to article [1].
  
  There is no explanation however of some of the parts of the MapReduce
  framework that are later used in the pseudo-code. For example the
  initialization and closure of mappers and reducers is not explained - when
  those methods are invoked, what is their purpose, etc. There are also obvious
  typing error that are inadmissible for published articles.  
% subsection MapReduce (end)

\subsection{Graph algorithms} % (fold)
\label{sub:Graph algorithms}
  In this section, the reader is introduced with a definition of a directed
  graph. Also the type of graph algorithms that will be implementable by the
  described design pattern is provided.

  Finally, a more precise definition of the example Page-Ranking algorithm is 
  provided. Page-Ranking calculates the importance of vertex $v_i$ in a graph,
  based on the graph structure. It computes the probability of reaching that 
  vertex through a random walk in the graph. The probability to reach vertex
  $v_i$ (denoted as $P_i$) is the sum of the probabilities to reach some of 
  the vertexes $v_j$, for which there is an edge ($v_j, v_i$) divided by the 
  number of outgoing edges from $v_j$. As there is no way to know $P_j$ that is
  needed to compute $P_i$ in the example above, a different approach is used - 
  we start with some probabilities and iteratively refine those probabilities,
  using the probabilities from the previous iteration, until they seem to
  remain unchanged during some iteration. Then we stop.
% subsection Graph algorithms (end)

\subsection{Earlier work} % (fold)
\label{sub:Earlier work}
  In this section, the authors present an direct implementation of the
  Page-Ranking algorithm in MapReduce framework. The idea of this implementation
  is for each edge of the graph to calculate the probability to follow it in
  a random walk of the graph and sum those probability for the end vertexes. 
  During the process the edges are also emitted (declared as partial result that
  we will later need) so that they can be used again later. During the reduce
  phase the probabilities for each vertex $v_i$ are added and all the edges that
  have it as a start vertex, are updated to have the correct probability to be
  on vertex $v_i$.
  
  The description is good, however the authors are inconsistent with names on
  places. At places some variable name is used for one thing and at others
  places it is used for something completely different. Still the overall
  description of the "Basic implementation" is easily understandable.  

  After the description of the basic implementation, a Schimmy implementation is
  provided. The basic idea of this new implementation is to skip the part of 
  emitting the graph topology as it does not change. To do this the graph is 
  partitioned and each reducer works on only one partition. During the reduce
  phase the partitions are read from the local disk. Each vertex that belongs
  to the partition is updated. For easier update a clever way is invented - 
  the vertexes are sorted as well as the edges in the partition. With only one
  pass through all the vertexes from the partition and the edges, the new 
  probabilities are calculated and updated on the corresponding edges. For more
  information how this is done see [1].

  The explanation of the Schimmy implementation is understandable and its 
  advantage over the basic implementation is clearly stated. 
% subsection Earlier work (end)

\subsection{Map-Based Graph Analysis} % (fold)
\label{sub:Map-Based Graph Analysis}
  In this section the authors provide more details how their approach is better
  then the Schimmy implementation. After that the explanation of the proposed
  implementation is given.

  The proposed solution consists of only map stage (there is no reduce stage). 
  This avoids the need of shuffling and sorting of partial results and reduces
  the communication cost of the algorithm. As in Schimmy a parallel merge-join
  is used (see [1] for more details). The difference with the already described
  approach is that the merge is not between a partition of the graph and 
  partial results for each vertex, but between the "partition of the graph and 
  global table that contains the partial results associated with all nodes"[1].
  Another difference is that the partitioning is done in a more sophisticated 
  manner - edges are not divided randomly among the different partitions, but
  instead it is necessary that all the edges with the same destination go in 
  the same partition. The partitions are sorted by the source vertex of the 
  edges. 

  The algorithm also uses a global binary file that is placed on the DFS and it
  contains the partial results of each node after the end of each iteration. 
  This file is sorted by the vertexes' IDs.

  There are some necessary pre-computations for the algorithm.

  Here is what the mapper does - for each destination vertex n in the partition
  $P_i$ (note that they are sorted in increasing order in the file) and each
  edge (from, to) from the same partition, if n is equal to from, then we use
  the vertex to update a in memory representation of the probability to reach
  vertex \emph{to}. As both sets are sorted, we don't need for each edge to go 
  through all the vertexes - we can do the same with only one pass through 
  all the vertexes and one pass through all the edges of the partition. In the
  end of the iteration, the memory representation of the probabilities is 
  written to the disk. As it is impossible two partitions to contain equal 
  destination vertexes, we know that only one mapper will calculate the 
  probability to reach certain node.

  We already discussed the good parts of this algorithm. The authors provide
  some experiments that show that on the overall their approach improves the 
  time that is necessary for the computations by 10\%. They however do not say
  anything about possible crashes and other weaknesses that their approach
  might have. One of the strongest disadvantages of the proposed approach is the problem with
  node failures. The use of MapReduce framework by no mean assures that there can
  not be any problems with a computational node crash. In such case using the proposed
  design pattern, it is practically impossible to continue the calculations as 
  there are side effects from the execution of the mappers (which is not handled
  by any mean by the framework) and we can not know the state of this sider
  effect (writing to a file on the DFS). Imagine that a computational node 
  crashes. By the time we know about this crash, it is possible that some of the
  other nodes have finished the map task for an input slot (all the tasks that
  should be executed by the same mapper) and as a consequence they might have
  written the new probabilities. In the context of page-ranking maybe this is not a 
  serious problem (even though it deviates from the correct algorithm), however
  it might be for some other similar iterative algorithm and the authors should
  have warned about it.
% subsection Map-Based Graph Analysis (end)

In the experiments part the authors provide information about a simulations
that they performed to test their approach. However there are not enough details
for example about what were the settings of Hadoop for the different
implementations. They said in the paper that they have set the replication
factor to 1 for their implementation. Was the same setting used for the other
implementations as well and what is the impact of this to the overall 
performance? Those questions were not answered.

To sum up, the paper is clearly written in most of its parts and it presents
some good ideas of ways to improve the efficiency of MapReduce graph algorithms,
however there is no account for the possible bad effects of the use of the 
proposed design pattern, there is no detailed information about the experiments
that showed that there is an improvement, there were typing errors and one of
the pseudo-code blocks is either wrong, or poorly explained. 

% section Paper Content (end)

\end{document}
