\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english,bulgarian]{babel}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{listings}
\usepackage{xcolor}

\oddsidemargin 0mm
\evensidemargin 0mm
\topmargin 0mm
\textheight 216mm
\textwidth 165mm

\pagestyle{fancy}
\fancyhf{}
\fancyhead[CE,CO]{Summary of \emph{"Map-Based Graph Analysis on MapReduce"}}
\fancyfoot[CE,CO]{\thepage}

\lstset{basicstyle=\ttfamily,
  mathescape=true,
  escapeinside=||,
  emph={Task, Operation, for, each, if, else, endif, endfor, while, then, When, do, repeat, until, endrepeat},
  emphstyle={\color{gray}\bfseries\itshape}}

\def\CC {{\mathbb C}}        % Complex numbers
\def\RR {{\mathbb R}}        % Real numbers
\def\ZZ {{\mathbb Z}}        % Integers
\def\NN {{\mathbb N}}        % Positive integers
\def\be  {\begin{eqnarray}}  % Begin of math formula with number
\def\ee  {\end{eqnarray}}    % End of math formula with number
\def\ben {\begin{eqnarray*}} % Begin of math formula without number 
\def\een {\end{eqnarray*}}   % End of math formula with number 
\newcommand{\hr}{\rule{\linewidth}{0.1mm}}
\newcommand{\bighs}{\hspace{15pt}}
\newcommand{\hs}{\hspace{10pt}}
\renewcommand{\tilde}{\overset{-}}

\newenvironment{itemize*}{
  \begin{itemize}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
}{
  \end{itemize}
}

\newenvironment{enumerate*}{
  \begin{enumerate}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
}{
  \end{enumerate}
}

\newtheoremstyle{plain}{1pt}{0pt}{}{}{}{}{.5em}{\thmname{\textbf{#1}}:\thmnote{#3}}

\theoremstyle{plain}
\renewcommand\thesection{\Roman{section}.}
\renewcommand\thesubsection{\thesection\Roman{subsection}.}
\renewcommand\thesubsubsection{\thesubsection\Roman{subsubsection}.}

% Makes the size of the symbols in the formulas fixed (not getting smaller)
\everymath{\displaystyle}

\begin{document}

\section*{\centering
  Map-Based Graph Analysis on MapReduce - summary \\
  Ivaylo Petrov 
}

\hr

%\section{Paper Content} % (fold)
%\label{sec:Paper Content}
The article that is going to be assessed is \emph{Map-Based Graph Analysis on
MapReduce} by Upa Gupta and Leonidas Fegaras, published in 2013. It provides
information about data processing using MapReduce framework (sometimes the open
source framework Hadoop is referred to instead of MapReduce as some of its 
specifics are employed) and more specifically big graph processing that
requires iterative graph algorithms. The authors of the article believe that
there is a need for efficient design pattern for solving such class of
problems, as more and more graph data is generated and the size of those graphs
is growing. They propose such a design pattern based on separation of the
immutable graph topology from the graph analysis results. This design pattern
can be applied to a large range of problems.  

%\subsection{Introduction} % (fold)
%\label{sub:Introduction}
  The authors point out, that in most cases the element that has the greatest
  impact on the performance of the algorithms that use Hadoop, is the
  communication time. Although there have been previous attempts to improve
  those aspects of the algorithms (introducing some new design patters for
  example), there is still possibility for improvement.  

  The authors use an example to describe their idea about the design pattern
  and to illustrate how it can be used in practice. The example is the
  page-ranking algorithm. Page-Ranking calculates the importance of vertex
  $v_i$ in a graph, based on the graph structure. It computes the probability
  of reaching that vertex through a random walk in the graph. The probability
  to reach vertex $v_i$ (denoted as $P_i$) is the sum of the probabilities to
  reach some of the vertices $v_j$, for which there is an edge ($v_j, v_i$)
  divided by the number of outgoing edges from $v_j$. As there is no way to
  know $P_j$ that is needed to compute $P_i$ in the example above, a different
  approach is used - we start with some probabilities and iteratively refine
  those probabilities, using the probabilities from the previous iteration,
  until they seem to remain unchanged during some iteration. Then we stop.
  
  Although the main focus in this paper is on this particular algorithm, the
  proposed design pattern can be also applied to many other problems. As
  expected the improvement has its price and some restrictions for cases where
  it can be applied. The restriction of the proposed design pattern is that it
  only works for \emph{"iterative graph algorithms on directed graphs where
    partial results associated with nodes can be improved at each
  iteration"}[1]. In other words only data about the notes is computed and
  stored and the algorithm should be trying to optimize it on each iteration. 
  There are many situations where such algorithms are needed and as a
  consequence the possible improvement is very important and valuable.  

  Even though the proposed algorithm is presented in more details later, a rough
  explanation will be given, in order the reader to attain a better
  understanding of the explanations that are to follow.

  The idea of the algorithm is to divide the graph in several parts (a multiple
  of the number of computational nodes in most cases). Every part is processed
  by a single node that is always the same, i.e. it does not change during the
  different iterations (at least this is the expectation). This way some data
  that is required for the computations (the graph topology that the node should
  process) can be made local for the node and the cost of the communication can
  be decreased. As the computation of each node are dependent on the
  computations of all the nodes during the previous iteration, all the result
  from the previous iterations are written on the distributed file system and
  thus every node can access it for its needs. This way the need of reducers is
  eliminated and as a consequence, it is possible to decrease the communication
  as shuffling of data is also skipped.  
% subsection Introduction (end)

%\subsection{Related work} % (fold)
%\label{sub:Related work}
  The authors present different previous attempts for solving the same or a 
  similar problem and give precise information what is the disadvantage of each
  of them.
% subsection Related work (end)

%\subsection{MapReduce} % (fold)
%\label{sub:MapReduce}
  In the MapReduce part of the article, the authors describe the MapReduce
  framework - how it works, when some actions are executed and other details.
  For more information, please refer to article [1]. There is no explanation,
  however, of some of the parts of the MapReduce framework that are later used
  in the pseudo-code. For example the initialization and closure of mappers and
  reducers is not explained - when those methods are invoked, what is their
  purpose, etc. There are also obvious typing errors that are inadmissible for
  published articles.  
% subsection MapReduce (end)

%\subsection{Graph algorithms} % (fold)
%\label{sub:Graph algorithms}
% subsection Graph algorithms (end)

%\subsection{Earlier work} % (fold)
%\label{sub:Earlier work}
  After that the authors present an direct implementation of the
  Page-Ranking algorithm in MapReduce framework. The idea of this implementation
  is for each edge of the graph to calculate the probability to follow it in
  a random walk of the graph and sum those probability for the destination
  vertices. During the process the edges are also emitted (declared as partial
  result that we will be needed later) so that they can be used again after
  that. During the reduce phase the probabilities for each vertex $v_i$ are
  added and all the edges that have it as a start vertex, are updated to have
  the correct probability to be on vertex $v_i$.
  
  The provided description of the basic implementation is good, however the
  authors are inconsistent with names at some places. Sometimes one variable
  name is used for one thing and after that it is used for something completely
  different. Still the overall description of the "Basic implementation" is
  easily understandable.  

  After that a Schimmy implementation is provided. The explanation is
  understandable and its advantages over the basic implementation are clearly
  stated. The idea of this Schimmy implementation is to skip the part of
  emitting the graph topology as it does not change. To do this the graph is
  partitioned and each reducer (a reducer instance working on one group of
  keys) works on only one partition. During the reduce phase the partitions are
  read from the local disk. Each vertex that belongs to the partition is
  updated.  For easier update a clever way is invented - the vertices are
  sorted as well as the edges in the partition. With only one pass through all
  the vertices from the partition and the edges, the new probabilities are
  calculated and updated on the corresponding edges. For more information how
  this is done see [1]. 
% subsection Earlier work (end)

%\subsection{Map-Based Graph Analysis} % (fold)
%\label{sub:Map-Based Graph Analysis}
  The solution proposed by the authors of the article consists of only map
  stage (there is no reduce stage).  This avoids the need of shuffling and
  sorting of partial results and reduces the communication cost of the
  algorithm. As in Schimmy a parallel merge-join is used (see [1] for more
  details). The difference with the already described approach is that the
  merge is not between a partition of the graph and partial results for each
  vertex, but between the "partition of the graph and global table that
  contains the partial results associated with all nodes"[1]. Another
  difference is that the partitioning is done in a more sophisticated manner -
  edges are not divided randomly among the different partitions, but instead it
  is necessary that all the edges with the same destination go in the same
  partition. The partitions are sorted by the source vertex of the edges. 

  The algorithm also uses a global binary file that is placed on the DFS and it
  contains the partial results of each node after the end of each iteration. 
  This file is sorted by the vertices' IDs. There are some necessary
  pre-computations for the algorithm.  

  Here is what the mapper does - for each destination vertex \emph{n} in the
  partition $P_i$ (note that they are sorted in increasing order in the file)
  and each edge \emph{(from, to)} from the same partition, if \emph{n} is equal
  to \emph{from}, then we use the vertex \emph{form} and the probability to
  get on it from the previous iteration, to update a in memory representation
  of the probability to reach vertex \emph{to}. As both sets are sorted, we
  don't need for each edge to go through all the vertices - we can do the same
  with only one pass through all the vertices and one pass through all the
  edges of the partition. In the end of the task (when the mapper is closing),
  the memory representation of the probabilities is written to the disk. As it
  is impossible that two partitions contain equal destination vertices, we
  know that only one mapper will calculate the probability to reach certain
  node and there will be no problem to merge the results from the different
  mappers. 

  We already discussed the good parts of this algorithm. The authors provide
  some experiments that show that on the overall their approach improves the 
  time that is necessary for the computations by 10\%. They however do not say
  anything about possible crashes and other weaknesses that their approach
  might have. One of the strongest disadvantages of the proposed approach is
  the problem with node failures. The use of MapReduce framework by no mean
  assures that there can not be any problems with a computational node crash.
  In such case, using the proposed design pattern, it is practically impossible
  to continue the calculations as there are side effects from the execution of
  the mappers (which is not handled by any mean by the framework) and we can
  not know the state of those side effects (writing to a file on the DFS).
  Imagine that a computational node crashes. By the time we know about this
  crash, it is possible that some of the other nodes have finished the map task
  for an input slot (all the tasks that should be executed by the same mapper)
  and as a consequence they might have written the new probabilities. In this
  situation we can not rerun the same mapper from the crashed node on some other
  node, as some of its input might be changed. In the context of page-ranking
  maybe this is not a serious problem (even though it deviates from the correct
  algorithm), however it might be for some other similar iterative algorithm
  and the authors should have warned about it or at least they should have
  discussed it.  
% subsection Map-Based Graph Analysis (end)

In the experiments part the authors provide information about a simulations
that they performed to test their approach. However there are not enough details
for example about what were the settings of Hadoop for the different
implementations. They said in the paper that they have set the replication
factor of the DFS to 1 for their implementation. Was the same setting used for
the other implementations as well and what is the impact of this to the overall 
performance? Those questions were not answered.

To sum up, the paper is clearly written in most of its parts and it presents
some good ideas of ways to improve the efficiency of MapReduce graph algorithms,
however there is no account for the possible bad effects of the use of the 
proposed design pattern, there is no detailed information about the experiments
that showed that there is an improvement, there were typing errors and one of
the pseudo-code blocks is either wrong, or poorly explained. All those things
jeopardize the reliability of this paper.
% section Paper Content (end)

An improvement that can solve the problem with the crashing nodes could be the 
use of snapshots of the file which is written on the DFS. After each iteration
for example, the name of the file can be changed. This way the crash of a node
will not prevent a mapper from being rerun (it just need to have information 
about which snapshot to use). As this might lead to the use of too much space,
however, it might be better to preserve just the last few snapshots, that are
needed by some of the mappers (the mappers still have not computed the partial
results using the snapshots).

\section*{References:} % (fold)
\label{sec:References:}
[1] Upa Gupta and Leonidas Fegaras, "Map-Based Graph Analysis on MapReduce" in
  2013 IEEE International Conference on Big Data.
% section References: (end)

\end{document}
